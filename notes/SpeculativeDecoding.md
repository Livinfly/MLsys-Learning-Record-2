# Fast Inference from Transformers via Speculative Decoding

[论文地址](https://arxiv.org/abs/2211.17192)

## 动机

-   大型AR自回归模型推理慢
-   任务分为**难的**和**简单的**，简单的往往有更加高效执行的模型
-   大模型的推理的瓶颈，相比于**计算**，更在于**内存带宽、通信**，增加**并行计算**

### 先前工作

-   对模型结构改变大、结果一致性一般
-   没有针对**瓶颈**特别设计

## 贡献

-   用 **Speculative Sampling** 推测采样，把 **Speculative Execution** 推测执行推广到**随机设置**场景中
-   提出 **Speculative Decoding** 推测解码，加速AR自回归模型，且无需再训练

实验结果说明，

对假设推出的个别场景，进行实验验证，如 $c \approx 0$ 的情况，使用 n-gram 来对比验证的；

简单启发式的方法，在特定任务中的优势；

搭配非 AR 的草稿模型，减小开销；

随机生成的草稿模型，都对模型有尽管微乎其微的帮助，对应前面假设推导出来的公式。

**最大采样**普遍比**标准采样**的效果好。



提出设置超参数 **lenience values**，使得能以一定程度接受和原分布不同，来提高推理效率。

## 方法

由理想的 i.i.d. 假设推出的假设，（反推/验证）对于「草稿」的**接受条件**。

## 未来可能方向

实验中，近似模型只测试了与目标模型**相同架构**的小模型或统计模型，没有测试**不同架构**的。（可能有**分词器**不同的原因）

因为模型大小与预测准确性高低是有冲突的，那么，是不是可以去做类似于 MoE 的多专家模型，使用多个草稿小模型，去融合不同场景？

在分布式部署时，通信成本，分布式部署方案？

**论文中提到的**

$f\left(\gamma\right)=\frac{1-\alpha^{\gamma+1}}{(1-\alpha)(\gamma c+1)}$，使用动态预测推测解码长度 $\gamma$，最理想的情况能提升 60%，训练这样的一个模型去做。

和 Beam Search 结合。

## 总结

提出了不改变架构、不需要重训练、优化内存瓶颈的**推测解码**方法，支持 argmax, standard sampling，能保持输出结果分布和原始分布一致。

为了验证方法的通用、有效，不仅做了相关实验，且基于 i.i.d. 的简化假设，数学严谨的估计了方法性能与超参数的设计。

## 源码阅读

论文中给出的伪代码。

[民间个人实现](https://github.com/romsto/Speculative-Decoding)

vllm中涉及部分。